"""
Data Validation Script

Validates HDF5 files generated by the IBL and Allen preprocessing scripts.
Checks schema compliance, data integrity, and compatibility with
torch_brain.dataset.Dataset.

Usage:
    conda run -n poyo python scripts/validate_data.py --data_dir /path/to/processed
"""

import argparse
import logging
from pathlib import Path

import h5py
import numpy as np

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)


def validate_hdf5_file(filepath: Path) -> list[str]:
    """Validate a single HDF5 file for NeuroHorizon compatibility.

    Returns:
        List of error messages (empty if valid)
    """
    errors = []

    try:
        f = h5py.File(filepath, "r")
    except Exception as e:
        return [f"Cannot open file: {e}"]

    try:
        # 1. Check required groups
        required_groups = ["session", "domain", "spikes", "units"]
        for grp in required_groups:
            if grp not in f:
                errors.append(f"Missing required group: {grp}")

        if errors:
            f.close()
            return errors

        # 2. Check session.id
        session_grp = f["session"]
        if "id" not in session_grp:
            errors.append("Missing session/id")

        # 3. Check domain
        domain = f["domain"]
        if "start" not in domain or "end" not in domain:
            errors.append("Missing domain/start or domain/end")
        else:
            starts = domain["start"][:]
            ends = domain["end"][:]
            if len(starts) == 0 or len(ends) == 0:
                errors.append("Empty domain arrays")
            elif np.any(starts >= ends):
                errors.append("Domain start >= end detected")

        # 4. Check spikes
        spikes = f["spikes"]
        if "timestamps" not in spikes:
            errors.append("Missing spikes/timestamps")
        if "unit_index" not in spikes:
            errors.append("Missing spikes/unit_index")

        if "timestamps" in spikes and "unit_index" in spikes:
            ts = spikes["timestamps"][:]
            idx = spikes["unit_index"][:]

            if len(ts) != len(idx):
                errors.append(
                    f"Spike timestamps ({len(ts)}) and unit_index ({len(idx)}) length mismatch"
                )

            if len(ts) == 0:
                errors.append("No spikes in file")
            else:
                # Check timestamps are sorted
                if not np.all(np.diff(ts) >= 0):
                    errors.append("Spike timestamps are not sorted")

                # Check for NaN
                if np.any(np.isnan(ts)):
                    errors.append("NaN values in spike timestamps")

                # Check unit indices are valid
                n_units = len(f["units"]["id"][:])
                if np.any(idx < 0) or np.any(idx >= n_units):
                    errors.append(
                        f"Unit index out of range [0, {n_units}): "
                        f"min={idx.min()}, max={idx.max()}"
                    )

        # 5. Check units
        units = f["units"]
        if "id" not in units:
            errors.append("Missing units/id")
        else:
            unit_ids = units["id"][:]
            if len(unit_ids) == 0:
                errors.append("No units in file")
            # Check for duplicates
            unique_ids = set()
            for uid in unit_ids:
                uid_str = uid.decode() if isinstance(uid, bytes) else str(uid)
                if uid_str in unique_ids:
                    errors.append(f"Duplicate unit ID: {uid_str}")
                unique_ids.add(uid_str)

        # 6. Check train/valid/test domains if present
        for split in ["train_domain", "valid_domain", "test_domain"]:
            if split in f:
                split_grp = f[split]
                if "start" in split_grp and "end" in split_grp:
                    s = split_grp["start"][:]
                    e = split_grp["end"][:]
                    if np.any(s >= e):
                        errors.append(f"{split}: start >= end")

        # 7. Summary statistics
        if not errors:
            n_spikes = len(spikes["timestamps"][:])
            n_units = len(units["id"][:])
            ts = spikes["timestamps"][:]
            duration = float(ts[-1] - ts[0]) if n_spikes > 0 else 0
            avg_rate = n_spikes / duration / n_units if duration > 0 and n_units > 0 else 0
            logger.info(
                f"  VALID: {n_spikes:,} spikes, {n_units} units, "
                f"{duration:.1f}s, avg rate={avg_rate:.1f} Hz/unit"
            )

    except Exception as e:
        errors.append(f"Unexpected error: {e}")
    finally:
        f.close()

    return errors


def test_dataset_loading(data_dir: Path, max_files: int = 3):
    """Test loading HDF5 files through torch_brain.dataset.Dataset."""
    try:
        from torch_brain.dataset import Dataset

        logger.info(f"Testing Dataset loading from {data_dir}...")
        dataset = Dataset(dataset_dir=data_dir, keep_files_open=True)
        logger.info(f"  Loaded {len(dataset.recording_ids)} recordings")

        for rid in dataset.recording_ids[:max_files]:
            data = dataset.get_recording(rid)
            logger.info(
                f"  Recording '{rid}': "
                f"spikes.timestamps={len(data.spikes.timestamps)}, "
                f"units.id={len(data.units.id)}"
            )

        logger.info("  Dataset loading: PASS")
        return True

    except Exception as e:
        logger.error(f"  Dataset loading: FAIL - {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    parser = argparse.ArgumentParser(description="Validate NeuroHorizon HDF5 data files")
    parser.add_argument(
        "--data_dir",
        type=str,
        required=True,
        help="Directory containing HDF5 files to validate",
    )
    parser.add_argument(
        "--test_loading",
        action="store_true",
        help="Also test loading through torch_brain.dataset.Dataset",
    )
    args = parser.parse_args()

    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        logger.error(f"Data directory does not exist: {data_dir}")
        return

    h5_files = sorted(data_dir.glob("*.h5"))
    if not h5_files:
        logger.error(f"No .h5 files found in {data_dir}")
        return

    logger.info(f"Validating {len(h5_files)} HDF5 files in {data_dir}")

    valid_count = 0
    invalid_count = 0

    for filepath in h5_files:
        logger.info(f"Checking {filepath.name}...")
        errors = validate_hdf5_file(filepath)
        if errors:
            invalid_count += 1
            for err in errors:
                logger.error(f"  ERROR: {err}")
        else:
            valid_count += 1

    logger.info(f"\nValidation summary: {valid_count} valid, {invalid_count} invalid")

    if args.test_loading and valid_count > 0:
        test_dataset_loading(data_dir)


if __name__ == "__main__":
    main()
